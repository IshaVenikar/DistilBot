{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7iUi25RUD8r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb096647-872a-40c1-f209-757ee2675326"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.3.2\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=56d05d5951734c9aaabc45faea85d8d3acf46059f9bc85550db328d9991530af\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: sentencepiece, sentence_transformers\n",
            "Successfully installed sentence_transformers-2.2.2 sentencepiece-0.1.99\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade scikit-learn\n",
        "!pip install sentence_transformers\n",
        "!pip install textblob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY-J0nn_UD8t"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import numpy as np\n",
        "from textblob import TextBlob\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xnNYD0mUD8t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a79ce6-9256-42f1-8ea0-09a1b89d87ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bUhOl9TUD8u"
      },
      "outputs": [],
      "source": [
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        text = text.lower()\n",
        "\n",
        "        words = word_tokenize(text)\n",
        "\n",
        "        words = [self.lemmatizer.lemmatize(word) for word in words if word.isalnum() and word not in self.stop_words]\n",
        "\n",
        "        cleaned_text = \" \".join(words)\n",
        "\n",
        "        return cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JP7YGMsYUD8u"
      },
      "outputs": [],
      "source": [
        "class GenerateKeyword:\n",
        "    def __init__(self, num_topics=1, max_features=1000):\n",
        "        self.num_topics = num_topics\n",
        "        self.max_features = max_features\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(max_features=max_features)\n",
        "        self.lda_model = LatentDirichletAllocation(n_components=num_topics)\n",
        "        self.text_preprocessor = TextPreprocessor()\n",
        "\n",
        "    def generate_topics(self, paragraph):\n",
        "        preprocessed_paragraph = self.text_preprocessor.preprocess_text(paragraph)\n",
        "        tfidf_matrix = self.calculate_tfidf(preprocessed_paragraph)\n",
        "        topics = self.extract_topics(tfidf_matrix)\n",
        "\n",
        "        return topics\n",
        "\n",
        "    def calculate_tfidf(self, text):\n",
        "        tfidf_matrix = self.tfidf_vectorizer.fit_transform([text])\n",
        "        return tfidf_matrix\n",
        "\n",
        "    def extract_topics(self, tfidf_matrix):\n",
        "        lda_model = LatentDirichletAllocation(n_components=self.num_topics)\n",
        "        lda_topic_matrix = lda_model.fit_transform(tfidf_matrix)\n",
        "\n",
        "        topics = []\n",
        "        for topic_idx, topic in enumerate(lda_model.components_):\n",
        "            top_keywords_idx = topic.argsort()[-10:][::-1]\n",
        "            top_keywords = [self.tfidf_vectorizer.get_feature_names_out()[i] for i in top_keywords_idx]\n",
        "            topics.append(top_keywords)\n",
        "\n",
        "        return topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sv1_IUMdUD8u"
      },
      "outputs": [],
      "source": [
        "class QuestionAnswer:\n",
        "    def __init__(self, paragraph, user_request):\n",
        "        self.question = user_request\n",
        "        self.paragraph = paragraph\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.paragraph_vector = self.vectorizer.fit_transform([paragraph])\n",
        "        self.sentences = self._split_sentences(paragraph)\n",
        "\n",
        "    def _split_sentences(self, paragraph):\n",
        "        blob = TextBlob(paragraph)\n",
        "        return [str(sentence) for sentence in blob.sentences]\n",
        "\n",
        "    def embed_sentences(self, sentences):\n",
        "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        embeddings = model.encode(sentences)\n",
        "        return embeddings\n",
        "\n",
        "    def answer_question(self):\n",
        "        # print(\"Inside answer question, sentences: \")\n",
        "        # print(self.sentences)\n",
        "        # print(\"Inside answer question, question: \")\n",
        "        # print(self.question)\n",
        "        question_vector = self.embed_sentences([self.question])\n",
        "        sentence_vectors = self.embed_sentences(self.sentences)\n",
        "\n",
        "        similarities = cosine_similarity(question_vector, sentence_vectors)\n",
        "        most_similar_sentence_index = similarities.argmax()\n",
        "\n",
        "        answer = self.sentences[most_similar_sentence_index]\n",
        "        return answer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Summarization:\n",
        "  def __init__(self):\n",
        "    from transformers import pipeline\n",
        "    self.model = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")\n",
        "\n",
        "  def summarize(self,paragraph):\n",
        "    return self.model(paragraph)[0]['summary_text']"
      ],
      "metadata": {
        "id": "_n5Ic_FLzo5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***SUMMARIZATION AND KEYWORD GENERATION*** = Advancements in artificial intelligence (AI) have significantly impacted various industries. Machine learning algorithms, a subset of AI, enable computers to learn and make decisions without explicit programming. In healthcare, AI is being employed for diagnostics and personalized treatment plans. The financial sector utilizes AI for fraud detection and risk management. Autonomous vehicles leverage AI to enhance navigation and safety. Natural language processing (NLP) allows computers to understand and generate human-like text, improving chatbots and language translation. Despite the benefits, ethical considerations, such as bias in algorithms, pose challenges for the widespread adoption of AI technologies.\n",
        "bold text\n",
        "\n",
        "***QUESTION ANSWER*** = I remember when I first arrived in the United States. Even before the plane landed, the little windows in the airplane revealed snow and ice-covered houses and buildings.\n",
        "As I walked off the plane, cold air crept through the corrugated ramp that led to the airport terminal. Some people inside the airport were wearing big coats and hats, which I had seen on\n",
        "television, but never up close.\n",
        "I felt a little dizzy and needed to sit down, and then my cell phone rang. It was my Aunt Sophia. She was waiting for me outside in the passenger pick-up area, so I walked quickly to the exit,\n",
        "forgetting all about my luggage.\n",
        "When the sliding glass door opened to the outside, there was my aunt–a woman I hadn’t seen in over ten years–wearing a parka and waving her arms frantically in my direction."
      ],
      "metadata": {
        "id": "Tey0mtQr1bW2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ymUXjwZUD8u"
      },
      "outputs": [],
      "source": [
        "class IntentRecognition:\n",
        "    def __init__(self):\n",
        "        self.intent_keywords = {\n",
        "            \"Summarization\": [\"summarize\", \"summary\", \"brief\"],\n",
        "            \"Topic Generation\": [\"topic\", \"main idea\", \"key points\"],\n",
        "            \"Keyword Generation\": [\"keywords\"],\n",
        "            \"Question Answering\": [\"what\", \"why\", \"explain\", \"tell me about\",\"who\"]\n",
        "        }\n",
        "\n",
        "        self.input_paragraph = \"\"\n",
        "        self.user_request = \"\"\n",
        "\n",
        "    def extract_paragraph(self, user_input):\n",
        "        paragraph_pattern = r'\"(.*?)\"'\n",
        "        match = re.search(paragraph_pattern, user_input)\n",
        "\n",
        "        if match:\n",
        "            self.input_paragraph = match.group(1)\n",
        "            user_request_pattern = r'^(.*?)' + re.escape(self.input_paragraph) + r'(.*)$'\n",
        "            user_request_match = re.search(user_request_pattern, user_input)\n",
        "            if user_request_match:\n",
        "                self.user_request = user_request_match.group(1).strip() + user_request_match.group(2).strip()\n",
        "\n",
        "    def recognize_intent(self, user_input):\n",
        "        detected_intent = \"Unknown\"\n",
        "        self.extract_paragraph(user_input)\n",
        "\n",
        "        for intent, keywords in self.intent_keywords.items():\n",
        "            for keyword in keywords:\n",
        "                if keyword.lower() in self.user_request.lower():\n",
        "                    detected_intent = intent\n",
        "                    break\n",
        "\n",
        "        return detected_intent\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    recognizer = IntentRecognition()\n",
        "\n",
        "    user_input = input(\"Hey! My name is DistilBot. I can assist you with tasks related to Keyword Extraction and Summarization. I can also answer queries related to your paragraph. How can i help you today?\")\n",
        "    # print(user_input)\n",
        "    # print(type(user_input))\n",
        "    intent = recognizer.recognize_intent(user_input)\n",
        "    paragraph = recognizer.input_paragraph\n",
        "    user_request = recognizer.user_request[:-3]\n",
        "\n",
        "    # print(\"Detected Intent:\", intent)\n",
        "    # print(\"Input Paragraph:\", paragraph)\n",
        "    # print(\"User Request:\", user_request)\n",
        "    print(\"Result: \")\n",
        "\n",
        "    if intent == \"Question Answering\":\n",
        "      qa = QuestionAnswer(paragraph, user_request)\n",
        "      print(qa.answer_question())\n",
        "    elif intent == \"Keyword Generation\":\n",
        "      topic_generator = GenerateKeyword()\n",
        "      topics = topic_generator.generate_topics(paragraph)\n",
        "      print(topics)\n",
        "      # print(\"hello\")\n",
        "    elif intent == \"Summarization\":\n",
        "      s = Summarization()\n",
        "      print(s.summarize(paragraph))\n",
        "    else:\n",
        "      print(\"Please input correct task\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}